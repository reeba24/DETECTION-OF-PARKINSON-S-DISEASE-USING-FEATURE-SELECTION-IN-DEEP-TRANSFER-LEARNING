# -*- coding: utf-8 -*-
"""major_proj_rev_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yDRNZ1R9JuPtUvhGZ8AhcpyV3GTGSp0I
"""

!pip install catboost

#GENERAL
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#PATH PROCESS
import os
import os.path
from pathlib import Path
import glob

#IMAGE PROCESS
from PIL import Image
from tensorflow.keras.preprocessing import image  # Changed to tensorflow.keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Changed to tensorflow.keras
import cv2

#SCALER & TRANSFORMATION
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.utils import to_categorical  # Changed to tensorflow.keras
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers  # Changed to tensorflow.keras
from sklearn.preprocessing import LabelEncoder

#ACCURACY CONTROL
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score

#OPTIMIZER
from tensorflow.keras.optimizers import RMSprop, Adam  # Changed to tensorflow.keras

#MODEL LAYERS
from tensorflow.keras.models import Sequential  # Changed to tensorflow.keras
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, MaxPooling2D, \
    Permute, TimeDistributed, Bidirectional, GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D  # Changed to tensorflow.keras
from tensorflow.keras import models, layers  # Changed to tensorflow.keras
import tensorflow as tf
from tensorflow.keras.applications import VGG16, VGG19, InceptionV3  # Changed to tensorflow.keras
from tensorflow.keras import backend as K  # Changed to tensorflow.keras
from tensorflow.keras.utils import plot_model  # Changed to tensorflow.keras

#SKLEARN CLASSIFIER
from xgboost import XGBClassifier, XGBRegressor
from lightgbm import LGBMClassifier, LGBMRegressor
from catboost import CatBoostClassifier, CatBoostRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import Lasso
from sklearn.linear_model import LassoCV
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import ElasticNetCV

#IGNORING WARNINGS
from warnings import filterwarnings
filterwarnings("ignore", category=DeprecationWarning)
filterwarnings("ignore", category=FutureWarning)
filterwarnings("ignore", category=UserWarning)

from google.colab import drive


drive.mount('/content/drive')

import os


folder_path1 = '/content/drive/My Drive/testing'

files = os.listdir(folder_path1)
print(files)

import os

folder_path2 = '/content/drive/My Drive/training'

files = os.listdir(folder_path2)
print(files)

Spiral_Train_Path = Path("/content/drive/My Drive/training")
Spiral_Test_Path = Path("/content/drive/My Drive/testing")

print("Spiral Training Path:", Spiral_Train_Path)
print("Spiral Testing Path:", Spiral_Test_Path)

Spiral_Train_PNG_Path = list(Spiral_Train_Path.glob("**/*.png"))
Spiral_Test_PNG_Path = list(Spiral_Test_Path.glob("**/*.png"))

print("Training PNG files:", Spiral_Train_PNG_Path)
print("Testing PNG files:", Spiral_Test_PNG_Path)

Spiral_Train_PNG_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Spiral_Train_PNG_Path))
Spiral_Test_PNG_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Spiral_Test_PNG_Path))

"""TRANSFORMATION TO SERIES STRUCTURE"""

Spiral_Train_PNG_Path_Series = pd.Series(Spiral_Train_PNG_Path,name="PNG").astype(str)
Spiral_Train_PNG_Labels_Series = pd.Series(Spiral_Train_PNG_Labels,name="CATEGORY")

Spiral_Test_PNG_Path_Series = pd.Series(Spiral_Test_PNG_Path,name="PNG").astype(str)
Spiral_Test_PNG_Labels_Series = pd.Series(Spiral_Test_PNG_Labels,name="CATEGORY")

"""TRANSFORMATION TO DATAFRAME STRUCTURE"""

Main_Spiral_Train_Data = pd.concat([Spiral_Train_PNG_Path_Series,Spiral_Train_PNG_Labels_Series],axis=1)
print(Main_Spiral_Train_Data.head(-1))

"""# skewness"""

import pandas as pd
import numpy as np
from scipy.stats import skew
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
import os

# Function to calculate skewness for a single image
def calculate_image_skewness(image_path):
    image = Image.open(image_path).convert("L")  # Convert to grayscale
    pixel_values = np.array(image).flatten()    # Flatten to 1D array
    return skew(pixel_values)

# Analyze skewness for the dataset
def analyze_dataset_skewness(image_paths, labels, dataset_name):
    skewness_list = []
    for path in tqdm(image_paths, desc=f"Processing {dataset_name}"):
        skewness_list.append(calculate_image_skewness(path))

    # Create a DataFrame
    df = pd.DataFrame({
        "Image Path": image_paths,
        "Label": labels,
        "Skewness": skewness_list
    })

    # Summary statistics
    print(f"\n{dataset_name} Skewness Statistics:")
    print(df["Skewness"].describe())

    # Visualization
    plt.figure(figsize=(10, 5))
    plt.hist(df["Skewness"], bins=30, color='blue', alpha=0.7, label=f'{dataset_name} Skewness')
    plt.title(f'{dataset_name} Skewness Distribution')
    plt.xlabel('Skewness')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

    return df

# Paths and labels from your dataset
train_image_paths = Spiral_Train_PNG_Path
test_image_paths = Spiral_Test_PNG_Path
train_labels = Spiral_Train_PNG_Labels
test_labels = Spiral_Test_PNG_Labels

# Analyze training set
train_skewness_df = analyze_dataset_skewness(train_image_paths, train_labels, "Training Set")

# Analyze testing set
test_skewness_df = analyze_dataset_skewness(test_image_paths, test_labels, "Testing Set")

# Save skewness data
train_skewness_df.to_csv("train_skewness.csv", index=False)
test_skewness_df.to_csv("test_skewness.csv", index=False)

Main_Spiral_Test_Data = pd.concat([Spiral_Test_PNG_Path_Series,Spiral_Test_PNG_Labels_Series],axis=1)
print(Main_Spiral_Train_Data.head(-1))

"""shuffling"""

Main_Spiral_Train_Data = Main_Spiral_Train_Data.sample(frac=1).reset_index(drop=True)
Main_Spiral_Test_Data = Main_Spiral_Test_Data.sample(frac=1).reset_index(drop=True)
print(Main_Spiral_Train_Data.head(-1))
print("---"*20)
print(Main_Spiral_Test_Data.head(-1))



"""CLASSIFIERS"""

Spiral_New_JPG_Path = []
for i in range(0,72):
    x = cv2.imread(Main_Spiral_Train_Data["PNG"][i])
    x = np.array(x).astype("float32")
    x = x.mean()
    Spiral_New_JPG_Path.append(x)

Spiral_New_JPG_Path_Series = pd.Series(Spiral_New_JPG_Path,name="PNG")

encode = LabelEncoder()
Spiral_New_JPG_Labels = encode.fit_transform(Main_Spiral_Train_Data["CATEGORY"])
Spiral_New_JPG_Labels_Series = pd.Series(Spiral_New_JPG_Labels,name="CATEGORY")
print(Spiral_New_JPG_Labels_Series)

Main_Spiral_New_Data = pd.concat([Spiral_New_JPG_Path_Series,Spiral_New_JPG_Labels_Series],axis=1)
print(Main_Spiral_New_Data)



"""# Training nd Testing

TRAIN & TEST
"""

x = Main_Spiral_New_Data[["PNG"]]
y = Main_Spiral_New_Data["CATEGORY"]
print(x.shape)

print(y.shape)

xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.1,random_state=42)
print(xTrain.shape)
print(xTest.shape)
print(yTrain.shape)
print(yTest.shape)

"""MODELS

# LGBM
"""

#lgbm

import numpy as np
import warnings
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, make_scorer
from lightgbm import LGBMClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Suppress warnings
warnings.filterwarnings("ignore")

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define scoring metric
scorer = make_scorer(accuracy_score)

# LightGBM Classifier
lgbm = LGBMClassifier(verbose=-1)
lgbm_scores = cross_val_score(lgbm, X, y, scoring=scorer, cv=cv)

# Print accuracy
print("LightGBM Accuracy:", round(np.mean(lgbm_scores), 4))

import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix
from lightgbm import LGBMClassifier
from sklearn.preprocessing import StandardScaler

# Suppress warnings
warnings.filterwarnings("ignore")

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define and train LightGBM Classifier
lgbm = LGBMClassifier(verbose=-1, n_estimators=100)  # 100 boosting rounds
lgbm.fit(X_train, y_train)

# Predict on test data
y_pred = lgbm.predict(X_test)

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - LightGBM")
plt.show()

"""# Catboost"""

#catboost

import numpy as np
import warnings
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, make_scorer
from catboost import CatBoostClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Suppress warnings
warnings.filterwarnings("ignore")

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define scoring metric
scorer = make_scorer(accuracy_score)

# CatBoost Classifier
catboost = CatBoostClassifier(silent=True)
catboost_scores = cross_val_score(catboost, X, y, scoring=scorer, cv=cv)

# Print accuracy
print("CatBoost Accuracy:", round(np.mean(catboost_scores), 4))

import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix
from catboost import CatBoostClassifier
from sklearn.preprocessing import StandardScaler

# Suppress warnings
warnings.filterwarnings("ignore")

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define and train CatBoost Classifier
catboost = CatBoostClassifier(silent=True, iterations=100)
catboost.fit(X_train, y_train)

# Predict on test data
y_pred = catboost.predict(X_test)

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - CatBoost")
plt.show()

"""# Resnet"""

#resnet

import numpy as np
import warnings
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.neural_network import MLPClassifier
from sklearn.base import BaseEstimator
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Suppress warnings
warnings.filterwarnings("ignore")

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define scoring metric
scorer = make_scorer(accuracy_score)

# Custom ResNet-like Model using MLP
class SimpleResNet(BaseEstimator):
    def __init__(self):
        self.model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=200, solver='adam', random_state=42)

    def fit(self, X, y):
        return self.model.fit(X, y)

    def predict(self, X):
        return self.model.predict(X)

    def score(self, X, y):
        return self.model.score(X, y)

# Train and evaluate ResNet
resnet = SimpleResNet()
resnet_scores = cross_val_score(resnet, X, y, scoring=scorer, cv=cv)

# Print accuracy
print("ResNet Accuracy:", round(np.mean(resnet_scores), 4))

import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.base import BaseEstimator
from sklearn.preprocessing import StandardScaler

# Suppress warnings
warnings.filterwarnings("ignore")

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Custom ResNet-like Model using MLP
class SimpleResNet(BaseEstimator):
    def __init__(self):
        self.model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=200, solver='adam', random_state=42)

    def fit(self, X, y):
        return self.model.fit(X, y)

    def predict(self, X):
        return self.model.predict(X)

# Train ResNet model
resnet = SimpleResNet()
resnet.fit(X_train, y_train)

# Predict on test data
y_pred = resnet.predict(X_test)

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - ResNet (MLPClassifier)")
plt.show()

"""IMAGE GENERATOR PROCESS

APPLYING GENERATOR
"""

Train_Generator = ImageDataGenerator(rescale=1./255,
                                    zoom_range=0.7,
                                    shear_range=0.7,
                                    rotation_range=50,
                                    horizontal_flip=True,
                                     brightness_range=[0.2,0.9],
                                     vertical_flip=True,
                                    validation_split=0.1)
Train_Spiral_Set = Train_Generator.flow_from_dataframe(dataframe=Main_Spiral_Train_Data,
                                                   x_col="PNG",
                                                   y_col="CATEGORY",
                                                   color_mode="grayscale",
                                                   class_mode="categorical",
                                                       subset="training")

Validation_Spiral_Set = Train_Generator.flow_from_dataframe(dataframe=Main_Spiral_Train_Data,
                                                   x_col="PNG",
                                                   y_col="CATEGORY",
                                                   color_mode="grayscale",
                                                   class_mode="categorical",
                                                       subset="validation")

Test_Spiral_Set = Train_Generator.flow_from_dataframe(dataframe=Main_Spiral_Test_Data,
                                                   x_col="PNG",
                                                   y_col="CATEGORY",
                                                   color_mode="grayscale",
                                                   class_mode="categorical")

print("TRAIN: ")
print(Train_Spiral_Set.class_indices)
print(Train_Spiral_Set.classes[0:5])
print(Train_Spiral_Set.image_shape)
print("---"*20)
print("VALIDATION: ")
print(Validation_Spiral_Set.class_indices)
print(Validation_Spiral_Set.classes[0:5])
print(Validation_Spiral_Set.image_shape)
print("---"*20)
print("TEST: ")
print(Test_Spiral_Set.class_indices)
print(Test_Spiral_Set.classes[0:5])
print(Test_Spiral_Set.image_shape)

"""TRANSFORMATION"""

example_Image = Main_Spiral_Train_Data["PNG"][30]
Load_Image = image.load_img(example_Image,target_size=(210,210))
Array_Image = image.img_to_array(Load_Image)
Array_Image = Array_Image.reshape((1,) + Array_Image.shape)
i = 0
for batch in Train_Generator.flow(Array_Image,batch_size=5):
    plt.figure(i)
    Image = plt.imshow(image.img_to_array(batch[0]))
    i += 1
    if i % 4 == 0:
        break
plt.show()



"""MODEL

CNN
"""

Call_Back_Early = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",patience=7,
                                                   mode="max")
Call_Back_Check = tf.keras.callbacks.ModelCheckpoint(monitor="val_accuracy",
                                                     save_best_only=True,
                                                     filepath=".keras")

Model_One = Sequential()


Model_One.add(Conv2D(2,(15,15),activation="relu",
                 input_shape=(256,256,1)))
Model_One.add(MaxPooling2D((2,2)))
Model_One.add(Dropout(0.2))

Model_One.add(Conv2D(4,(10,10),activation="relu",
                 strides=(2,2)))
Model_One.add(MaxPooling2D((2,2)))
Model_One.add(Dropout(0.2))


Model_One.add(Flatten())
Model_One.add(Dropout(0.5))
Model_One.add(Dense(512,activation="relu"))
Model_One.add(Dense(2,activation="softmax"))

Model_One.compile(optimizer=RMSprop(learning_rate=0.001),loss="categorical_crossentropy",metrics=["accuracy"])

from tensorflow.keras.callbacks import EarlyStopping

# Define the EarlyStopping callback
Call_Back_Early = EarlyStopping(
    monitor='val_loss',      # Monitoring the validation loss
    patience=10,             # Wait for 10 epochs without improvement before stopping
    restore_best_weights=True, # Restore the best weights found during training
    verbose=1                # Print when early stopping is triggered
)

# Other callbacks (if any) like model checkpoints can be added here:
# Example:
# Call_Back_Check = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

# Train the model for 50 epochs
CNN_Model_One = Model_One.fit(
    Train_Spiral_Set,               # Training dataset
    validation_data=Validation_Spiral_Set,  # Validation dataset
        # Pass in your callbacks
    epochs=50,                      # Set the number of epochs to 50
    verbose=1                       # Print training progress per epoch
)



"""ANN"""

import tensorflow as tf

Model_Two = tf.keras.models.Sequential([
    # inputs
    tf.keras.layers.Rescaling(1./255, input_shape=(113,)),
    tf.keras.layers.Flatten(),
    # hidden layers
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    # output layer
    tf.keras.layers.Dense(2, activation="softmax")
])

lossfunc = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

Model_Two.compile(optimizer='adam', loss=lossfunc, metrics=['accuracy'])

import tensorflow as tf

Model_Two = tf.keras.models.Sequential([
    # inputs
    tf.keras.layers.Rescaling(1./255, input_shape=(256, 256, 1)),  # Adjust input shape here
    tf.keras.layers.Flatten(),
    # hidden layers
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    # output layer
    tf.keras.layers.Dense(2, activation="softmax")
])

import numpy as np
import tensorflow as tf

# Example data (make sure to replace these with your actual datasets)
# Assuming Train_Spiral_Set and Validation_Spiral_Set are tuples of (features, labels)
# For instance, Train_Spiral_Set = (X_train, y_train) where y_train is one-hot encoded
X_train = np.random.rand(100, 256, 256, 1)  # Example feature data
y_train = tf.keras.utils.to_categorical(np.random.randint(2, size=(100,)), num_classes=2)  # One-hot encoded labels

X_val = np.random.rand(20, 256, 256, 1)  # Example validation feature data
y_val = tf.keras.utils.to_categorical(np.random.randint(2, size=(20,)), num_classes=2)  # One-hot encoded validation labels

# Example test data
X_test = np.random.rand(20, 256, 256, 1)  # Example test feature data
y_test = tf.keras.utils.to_categorical(np.random.randint(2, size=(20,)), num_classes=2)  # One-hot encoded test labels

# Define the model
Model_Two = tf.keras.models.Sequential([
    tf.keras.layers.Rescaling(1./255, input_shape=(256, 256, 1)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(2, activation="softmax")
])

# Compile the model
Model_Two.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # For one-hot encoded labels
    metrics=['accuracy']
)

# Fit the model
ANN_Model = Model_Two.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=10
)



"""PREDICTION PROCESS

CNN
"""

# Use the predict method to get the predicted probabilities
Prediction_One = Model_Two.predict(X_test)  # Make sure to use your test data (X_test)
Prediction_One_classes = Prediction_One.argmax(axis=-1)  # Get the class with the highest probability

# The probabilities are already in Prediction_One
Predict_Proba_One = Prediction_One  # This contains the predicted probabilities

print("Predicted classes:", Prediction_One_classes)
print("Predicted probabilities:", Predict_Proba_One)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

# Predict on the test data
y_pred_cnn = Model_One.predict(X_test)  # Replace X_test with your actual test dataset

# Convert predicted probabilities to class labels
y_pred_cnn_classes = np.argmax(y_pred_cnn, axis=1)

# Assuming y_test is one-hot encoded, convert it to class labels
y_test_classes = np.argmax(y_test, axis=1)  # Replace y_test with your actual labels

# Compute confusion matrix
cm_cnn = confusion_matrix(y_test_classes, y_pred_cnn_classes)

# Set global style for white background
sns.set(style="white")  # Ensures a white background

# Set figure with white background
plt.figure(figsize=(5, 5), dpi=80, facecolor='white')

# Customize the heatmap
sns.heatmap(
    cm_cnn,
    annot=True,
    fmt='d',
    cmap='magma',  # Color palette
    xticklabels=['Class 0', 'Class 1'],  # Predicted labels
    yticklabels=['Class 0', 'Class 1'],  # True labels
    cbar_kws={'label': 'Count'},  # Color bar label
)

# Labeling the axes and title
plt.xlabel('Predicted Labels')  # X-axis label
plt.ylabel('True Labels')  # Y-axis label
plt.title('Confusion Matrix for CNN')  # Title

# Display the plot
plt.show()



"""# ANN

ANN
"""

# Predict the probabilities using the predict method
Prediction_Two = Model_Two.predict(X_test)  # Use your test data, e.g., X_test
Prediction_Two_classes = Prediction_Two.argmax(axis=-1)  # Get the class with the highest probability

# The predicted probabilities are already in Prediction_Two
Predict_Proba_Two = Prediction_Two  # This contains the predicted probabilities

# Print the results
print("Predicted classes:", Prediction_Two_classes)
print("Predicted probabilities:", Predict_Proba_Two)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

# Assuming you have X_test and y_test
# Predict on the test data
y_pred_ann = Model_Two.predict(X_test)

# Convert the predicted probabilities to class labels (since the output is softmax)
y_pred_ann_classes = np.argmax(y_pred_ann, axis=1)

# Assuming y_test is one-hot encoded, convert it to class labels
y_test_classes = np.argmax(y_test, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_test_classes, y_pred_ann_classes)

# Set global style for white background
sns.set(style="white")  # This ensures the background is white

# Set figure with white background
plt.figure(figsize=(5, 5), dpi=80, facecolor='white')

# Customize the heatmap
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='magma',  # Color palette
    xticklabels=['Class 0', 'Class 1'],  # Predicted labels
    yticklabels=['Class 0', 'Class 1'],  # True labels
    cbar_kws={'label': 'Count'},  # Color bar label
)

# Labeling the axes and title
plt.xlabel('Predicted Labels')  # X-axis label
plt.ylabel('True Labels')  # Y-axis label
plt.title('Confusion Matrix for ANN')  # Title

# Display the plot
plt.show()



"""# Resnet CNN

resnet cnn
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from fastai import *
from fastai.vision import  *
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

BATCH_SIZE = 64
IMG_SIZE = 224
WORKERS = 0
DATA_PATH_STR = '../input/parkinsons-drawings/'
DATA_PATH_OBJ = Path(DATA_PATH_STR)

import os

# Define the path to your folder
fp = '/content/drive/My Drive/ParkinsonsDrawings'

# List files in the folder to confirm access
f = os.listdir(fp)
print(f)

from fastai.vision.all import *

# Define parameters
IMG_SIZE = 256  # Example image size
BATCH_SIZE = 32  # Example batch size
WORKERS = 4  # Number of workers for data loading
DATA_PATH_OBJ = '/content/drive/My Drive/ParkinsonsDrawings'  # Replace with your actual data path

# Function to get data augmentation transforms
def get_transforms():
    return [
        *aug_transforms(size=IMG_SIZE),  # Standard augmentations
        Normalize.from_stats(*imagenet_stats)  # Normalize using ImageNet stats
    ]

# Prepare data with transformations
tfms = get_transforms()  # Standard data augmentation

# Create DataLoaders
data = ImageDataLoaders.from_folder(
    DATA_PATH_OBJ,               # Get data from path object
    valid_pct=0.2,               # Separate 20% of data for validation set
    item_tfms=Resize(IMG_SIZE),  # Resize images
    batch_tfms=tfms,             # Apply transformations
    num_workers=WORKERS          # Number of workers for loading
)

# Now you can use `data` for training your model

('training DS size:', len(data.train_ds), 'validation DS size:' ,len(data.valid_ds))

"""# Prediction"""

data.show_batch(nrows=4, figsize=(10,8))

learn = cnn_learner(data, models.resnet34, metrics=accuracy, model_dir='/tmp/models')

learn.fit_one_cycle(10)

interp = ClassificationInterpretation.from_learner(learn)

losses,idxs = interp.top_losses()
interp.plot_top_losses(15, figsize=(20,20))
#perdiction/actual/loss/probability

import matplotlib.pyplot as plt

plt.figure(figsize=(5, 5), dpi=60, facecolor='white')

interp.plot_confusion_matrix(figsize=(5, 5), dpi=60)

plt.title('Confusion Matrix for ResNetCNN', fontsize=14)
plt.show()

#Spiral_Train_Path = Path("/content/drive/My Drive/training")
#Spiral_Test_Path = Path("/content/drive/My Drive/testing")

"""# SEVERITY

SEVERITY PREDICTION

SPIRAL - TRAINING
"""

min_skew = np.min(features)
max_skew = np.max(features)

simulated_updrs = 100 * (features - min_skew) / (max_skew - min_skew)

labels = []

for score in simulated_updrs:
    if score <= 32:
        labels.append("Mild")
    elif score <= 58:
        labels.append("Moderate")
    else:
        labels.append("Severe")

def map_skewness_to_severity(skew_val, min_val=-4.380180, max_val=-2.438485):
    # Normalize to 0–100
    normalized = (skew_val - min_val) / (max_val - min_val) * 100

    if normalized <= 32:
        return "Mild"
    elif 33 <= normalized <= 58:
        return "Moderate"
    else:
        return "Severe"

import os
import cv2
import numpy as np
from scipy.stats import skew
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

base_path = "/content/drive/My Drive/training"
healthy_path = os.path.join(base_path, "healthy")
parkinson_path = os.path.join(base_path, "parkinson")

def extract_features(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    if img is None:
        print(f"Error: Unable to load image {image_path}")
        return None

    img = cv2.resize(img, (128, 128))
    img_flat = img.flatten()
    img_skewness = skew(img_flat)
    return img_skewness

features = []
labels = []

for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(1)

for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(0)

features = np.array(features).reshape(-1, 1)
labels = np.array(labels)

kmeans = KMeans(n_clusters=3, random_state=42)
severity_labels = kmeans.fit_predict(features)

severity_mapping = {0: "Mild", 1: "Moderate", 2: "Severe"}
severity_labels = np.array([severity_mapping[label] for label in severity_labels])

scaler = StandardScaler()
X_train = scaler.fit_transform(features)

severity_numeric_mapping = {"Mild": 0, "Moderate": 1, "Severe": 2}
y_train = np.array([severity_numeric_mapping[label] for label in severity_labels])

y_train = to_categorical(y_train, num_classes=3)

model = Sequential([
    Dense(128, activation='relu', input_shape=(1,)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=20, batch_size=8, validation_split=0.2)

def predict_severity(image_path):
    skewness = extract_features(image_path)
    if skewness is None:
        return "Error: Unable to load image"

    skewness = scaler.transform([[skewness]])
    prediction = model.predict(skewness)
    severity_index = np.argmax(prediction)
    severity_class = {0: "Mild", 1: "Moderate", 2: "Severe"}[severity_index]
    return severity_class

all_predictions = []

for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    predicted_severity = predict_severity(img_path)
    all_predictions.append((img_name, "Parkinson", predicted_severity, img_path))

for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    all_predictions.append((img_name, "Healthy", "Healthy", img_path))

for prediction in all_predictions:
    img_name, img_type, result, img_path = prediction
    img = cv2.imread(img_path)

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    img_resized = cv2.resize(img_rgb, (128, 128))

    plt.figure(figsize=(4, 4))
    plt.imshow(img_resized)
    plt.title(f"Image: {img_name}, Type: {img_type}, Predicted Severity: {result}")
    plt.axis('off')
    plt.show()



"""SPIRAL -TESTING

testing images
"""

import os
import cv2
import numpy as np
from scipy.stats import skew
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Set paths
base_path = "/content/drive/My Drive/testing"
healthy_path = os.path.join(base_path, "healthy")
parkinson_path = os.path.join(base_path, "parkinson")

# Function to extract skewness feature
def extract_features(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Check if image is loaded correctly
    if img is None:
        print(f"Error: Unable to load image {image_path}")
        return None

    img = cv2.resize(img, (128, 128))  # Resize for consistency
    img_flat = img.flatten()
    img_skewness = skew(img_flat)  # Compute skewness
    return img_skewness

# Extract features from Parkinson and Healthy images
features = []
labels = []

# Extract features for Parkinson images
for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(1)  # Parkinson label

# Extract features for Healthy images
for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(0)  # Healthy label

# Convert features and labels to numpy arrays
features = np.array(features).reshape(-1, 1)
labels = np.array(labels)

# Apply K-Means clustering for severity classification (if needed)
kmeans = KMeans(n_clusters=3, random_state=42)
severity_labels = kmeans.fit_predict(features)

# Map cluster labels to severity levels
severity_mapping = {0: "Mild", 1: "Moderate", 2: "Severe"}
severity_labels = np.array([severity_mapping[label] for label in severity_labels])

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(features)

# Convert severity labels to numerical format
severity_numeric_mapping = {"Mild": 0, "Moderate": 1, "Severe": 2}
y_train = np.array([severity_numeric_mapping[label] for label in severity_labels])

# Convert labels to categorical format for ANN training
y_train = to_categorical(y_train, num_classes=3)

# Build ANN model
model = Sequential([
    Dense(128, activation='relu', input_shape=(1,)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')  # Output layer for 3 classes
])

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=20, batch_size=8, validation_split=0.2)

# Function to predict severity of a new image
def predict_severity(image_path):
    skewness = extract_features(image_path)
    if skewness is None:
        return "Error: Unable to load image"

    skewness = scaler.transform([[skewness]])  # Normalize
    prediction = model.predict(skewness)
    severity_index = np.argmax(prediction)
    severity_class = {0: "Mild", 1: "Moderate", 2: "Severe"}[severity_index]
    return severity_class

# Predict severity for all images in the "parkinson" and "healthy" folders
all_predictions = []

# For Parkinson images
for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    predicted_severity = predict_severity(img_path)
    all_predictions.append((img_name, "Parkinson", predicted_severity, img_path))

# For Healthy images
for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    all_predictions.append((img_name, "Healthy", "Healthy", img_path))

# Display images along with their corresponding results
for prediction in all_predictions:
    img_name, img_type, result, img_path = prediction
    img = cv2.imread(img_path)

    # Convert BGR to RGB (for proper display with matplotlib)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Resize the image to a smaller size (e.g., 128x128 pixels)
    img_resized = cv2.resize(img_rgb, (128, 128))

    # Create the plot with image and result
    plt.figure(figsize=(4, 4))  # Adjust figure size for better display
    plt.imshow(img_resized)
    plt.title(f"Image: {img_name}, Type: {img_type}, Predicted Severity: {result}")
    plt.axis('off')  # Hide axis
    plt.show()



"""WAVES - TRAINING"""

import os
import cv2
import numpy as np
from scipy.stats import skew
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Set paths
base_path = "/content/drive/My Drive/ParkinsonsDrawings/wave/training"
healthy_path = os.path.join(base_path, "healthy")
parkinson_path = os.path.join(base_path, "parkinson")

# Function to extract skewness feature
def extract_features(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Check if image is loaded correctly
    if img is None:
        print(f"Error: Unable to load image {image_path}")
        return None

    img = cv2.resize(img, (128, 128))  # Resize for consistency
    img_flat = img.flatten()
    img_skewness = skew(img_flat)  # Compute skewness
    return img_skewness

# Extract features from Parkinson and Healthy images
features = []
labels = []

# Extract features for Parkinson images
for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(1)  # Parkinson label

# Extract features for Healthy images
for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(0)  # Healthy label

# Convert features and labels to numpy arrays
features = np.array(features).reshape(-1, 1)
labels = np.array(labels)

# Apply K-Means clustering for severity classification (if needed)
kmeans = KMeans(n_clusters=3, random_state=42)
severity_labels = kmeans.fit_predict(features)

# Map cluster labels to severity levels
severity_mapping = {0: "Mild", 1: "Moderate", 2: "Severe"}
severity_labels = np.array([severity_mapping[label] for label in severity_labels])

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(features)

# Convert severity labels to numerical format
severity_numeric_mapping = {"Mild": 0, "Moderate": 1, "Severe": 2}
y_train = np.array([severity_numeric_mapping[label] for label in severity_labels])

# Convert labels to categorical format for ANN training
y_train = to_categorical(y_train, num_classes=3)

# Build ANN model
model = Sequential([
    Dense(128, activation='relu', input_shape=(1,)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')  # Output layer for 3 classes
])

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=20, batch_size=8, validation_split=0.2)

# Function to predict severity of a new image
def predict_severity(image_path):
    skewness = extract_features(image_path)
    if skewness is None:
        return "Error: Unable to load image"

    skewness = scaler.transform([[skewness]])  # Normalize
    prediction = model.predict(skewness)
    severity_index = np.argmax(prediction)
    severity_class = {0: "Mild", 1: "Moderate", 2: "Severe"}[severity_index]
    return severity_class

# Predict severity for all images in the "parkinson" and "healthy" folders
all_predictions = []

# For Parkinson images
for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    predicted_severity = predict_severity(img_path)
    all_predictions.append((img_name, "Parkinson", predicted_severity, img_path))

# For Healthy images
for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    all_predictions.append((img_name, "Healthy", "Healthy", img_path))

# Display images along with their corresponding results
for prediction in all_predictions:
    img_name, img_type, result, img_path = prediction
    img = cv2.imread(img_path)

    # Convert BGR to RGB (for proper display with matplotlib)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Resize the image to a smaller size (e.g., 128x128 pixels)
    img_resized = cv2.resize(img_rgb, (128, 128))

    # Create the plot with image and result
    plt.figure(figsize=(4, 4))  # Adjust figure size for better display
    plt.imshow(img_resized)
    plt.title(f"Image: {img_name}, Type: {img_type}, Predicted Severity: {result}")
    plt.axis('off')  # Hide axis
    plt.show()



"""WAVES - TESTING"""

import os
import cv2
import numpy as np
from scipy.stats import skew
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Set paths
base_path = "/content/drive/My Drive/ParkinsonsDrawings/wave/testing"
healthy_path = os.path.join(base_path, "healthy")
parkinson_path = os.path.join(base_path, "parkinson")

# Function to extract skewness feature
def extract_features(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Check if image is loaded correctly
    if img is None:
        print(f"Error: Unable to load image {image_path}")
        return None

    img = cv2.resize(img, (128, 128))  # Resize for consistency
    img_flat = img.flatten()
    img_skewness = skew(img_flat)  # Compute skewness
    return img_skewness

# Extract features from Parkinson and Healthy images
features = []
labels = []

# Extract features for Parkinson images
for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(1)  # Parkinson label

# Extract features for Healthy images
for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    skewness_value = extract_features(img_path)
    if skewness_value is not None:
        features.append(skewness_value)
        labels.append(0)  # Healthy label

# Convert features and labels to numpy arrays
features = np.array(features).reshape(-1, 1)
labels = np.array(labels)

# Apply K-Means clustering for severity classification (if needed)
kmeans = KMeans(n_clusters=3, random_state=42)
severity_labels = kmeans.fit_predict(features)

# Map cluster labels to severity levels
severity_mapping = {0: "Mild", 1: "Moderate", 2: "Severe"}
severity_labels = np.array([severity_mapping[label] for label in severity_labels])

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(features)

# Convert severity labels to numerical format
severity_numeric_mapping = {"Mild": 0, "Moderate": 1, "Severe": 2}
y_train = np.array([severity_numeric_mapping[label] for label in severity_labels])

# Convert labels to categorical format for ANN training
y_train = to_categorical(y_train, num_classes=3)

# Build ANN model
model = Sequential([
    Dense(128, activation='relu', input_shape=(1,)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')  # Output layer for 3 classes
])

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=20, batch_size=8, validation_split=0.2)

# Function to predict severity of a new image
def predict_severity(image_path):
    skewness = extract_features(image_path)
    if skewness is None:
        return "Error: Unable to load image"

    skewness = scaler.transform([[skewness]])  # Normalize
    prediction = model.predict(skewness)
    severity_index = np.argmax(prediction)
    severity_class = {0: "Mild", 1: "Moderate", 2: "Severe"}[severity_index]
    return severity_class

# Predict severity for all images in the "parkinson" and "healthy" folders
all_predictions = []

# For Parkinson images
for img_name in os.listdir(parkinson_path):
    img_path = os.path.join(parkinson_path, img_name)
    predicted_severity = predict_severity(img_path)
    all_predictions.append((img_name, "Parkinson", predicted_severity, img_path))

# For Healthy images
for img_name in os.listdir(healthy_path):
    img_path = os.path.join(healthy_path, img_name)
    all_predictions.append((img_name, "Healthy", "Healthy", img_path))

# Display images along with their corresponding results
for prediction in all_predictions:
    img_name, img_type, result, img_path = prediction
    img = cv2.imread(img_path)

    # Convert BGR to RGB (for proper display with matplotlib)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Resize the image to a smaller size (e.g., 128x128 pixels)
    img_resized = cv2.resize(img_rgb, (128, 128))

    # Create the plot with image and result
    plt.figure(figsize=(4, 4))  # Adjust figure size for better display
    plt.imshow(img_resized)
    plt.title(f"Image: {img_name}, Type: {img_type}, Predicted Severity: {result}")
    plt.axis('off')  # Hide axis
    plt.show()



#Spiral_Train_Path = Path("/content/drive/My Drive/training")
#Spiral_Test_Path = Path("/content/drive/My Drive/testing")

"""# ConvNeXt

novelty

convnext
"""

import os

dataset_path = "/content/drive/My Drive/training/parkinson"

if os.path.exists(dataset_path):
    print("Dataset path exists.")
    print("Contents:", os.listdir(dataset_path))  # List subdirectories
else:
    print("Dataset path does NOT exist. Check the path!")

from google.colab import drive


drive.mount('/content/drive')

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import ConvNeXtTiny
from tensorflow.keras.applications.convnext import preprocess_input
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

base_model = ConvNeXtTiny(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu")(x)
x = Dense(1, activation="sigmoid")(x)

model = Model(inputs=base_model.input, outputs=x)

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

train_folder = "/content/drive/My Drive/training"
batch_size = 32

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    train_folder,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode="binary",
    subset="training"
)

val_generator = train_datagen.flow_from_directory(
    train_folder,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode="binary",
    subset="validation"
)

model.fit(train_generator, validation_data=val_generator, epochs=5)

test_folder = "/content/drive/My Drive/testing"

def load_and_preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = preprocess_input(img_array)
    return np.expand_dims(img_array, axis=0)

# ✅ Step 10: Debug - Print Available Folders
print("Folders found in test dataset:", os.listdir(test_folder))  # Debugging print

true_labels = []
pred_probs = []
img_paths = []
img_names = []

for class_name in os.listdir(test_folder):
    class_folder = os.path.join(test_folder, class_name)

    if not os.path.isdir(class_folder):
        continue  # Skip non-folder items

    if class_name.lower() == "parkinson":
        label = 1  # Parkinson's
    elif class_name.lower() == "healthy":
        label = 0  # Healthy
    else:
        print(f"Skipping unknown folder: {class_name}")  # Debugging message
        continue  # Skip unknown folders

    for img_name in os.listdir(class_folder):
        img_path = os.path.join(class_folder, img_name)

        if img_path.lower().endswith((".jpg", ".jpeg", ".png")):
            img_array = load_and_preprocess_image(img_path)
            pred_prob = model.predict(img_array, verbose=0)[0][0]

            true_labels.append(label)
            pred_probs.append(pred_prob)
            img_paths.append(img_path)
            img_names.append(img_name)  # Store image name

true_labels = np.array(true_labels)
pred_probs = np.array(pred_probs)

threshold = 0.5


num_images = len(img_paths)
cols = 5  # Number of images per row
rows = (num_images // cols) + (num_images % cols > 0)

plt.figure(figsize=(20, rows * 4))

for i, img_path in enumerate(img_paths):
    img = image.load_img(img_path, target_size=(224, 224))

    plt.subplot(rows, cols, i + 1)
    plt.imshow(img)
    plt.axis("off")

    true_label = "Parkinson's" if true_labels[i] == 1 else "Healthy"
    pred_label = "Parkinson's" if pred_probs[i] > threshold else "Healthy"
    confidence = pred_probs[i]

    #  Include Image Name in Prediction
    plt.title(f"{img_names[i]}\nTrue: {true_label} | Pred: {pred_label}\nProb: {confidence:.2f}")

plt.tight_layout()
plt.show()

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import ConvNeXtTiny
from tensorflow.keras.applications.convnext import preprocess_input
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator


base_model = ConvNeXtTiny(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu")(x)
x = Dense(1, activation="sigmoid")(x)

model = Model(inputs=base_model.input, outputs=x)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])


train_folder = "/content/drive/My Drive/training"
batch_size = 32

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    train_folder, target_size=(224, 224), batch_size=batch_size, class_mode="binary", subset="training"
)

val_generator = train_datagen.flow_from_directory(
    train_folder, target_size=(224, 224), batch_size=batch_size, class_mode="binary", subset="validation"
)


model.fit(train_generator, validation_data=val_generator, epochs=5)


test_folder = "/content/drive/My Drive/testing"

def load_and_preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = preprocess_input(img_array)
    return np.expand_dims(img_array, axis=0)


print("Folders found in test dataset:", os.listdir(test_folder))

true_labels = []
pred_probs = []
img_paths = []
img_names = []


for class_name in os.listdir(test_folder):
    class_folder = os.path.join(test_folder, class_name)

    if not os.path.isdir(class_folder):
        continue

    if class_name.lower() == "parkinson":
        label = 1
    elif class_name.lower() == "healthy":
        label = 0
    else:
        print(f"Skipping unknown folder: {class_name}")
        continue

    for img_name in os.listdir(class_folder):
        img_path = os.path.join(class_folder, img_name)

        if img_path.lower().endswith((".jpg", ".jpeg", ".png")):
            img_array = load_and_preprocess_image(img_path)
            pred_prob = model.predict(img_array, verbose=0)[0][0]

            true_labels.append(label)
            pred_probs.append(pred_prob)
            img_paths.append(img_path)
            img_names.append(img_name)

true_labels = np.array(true_labels)
pred_probs = np.array(pred_probs)


threshold = 0.5
pred_labels = (pred_probs > threshold).astype(int)

correctly_classified_indices = np.where(true_labels == pred_labels)[0]


correct_img_paths = [img_paths[i] for i in correctly_classified_indices]
correct_img_names = [img_names[i] for i in correctly_classified_indices]
correct_true_labels = [true_labels[i] for i in correctly_classified_indices]
correct_pred_probs = [pred_probs[i] for i in correctly_classified_indices]


num_correct_images = len(correct_img_paths)
cols = 5
rows = (num_correct_images // cols) + (num_correct_images % cols > 0)

plt.figure(figsize=(20, rows * 4))

for i, img_path in enumerate(correct_img_paths):
    img = image.load_img(img_path, target_size=(224, 224))

    plt.subplot(rows, cols, i + 1)
    plt.imshow(img)
    plt.axis("off")

    true_label = "Parkinson's" if correct_true_labels[i] == 1 else "Healthy"
    pred_label = "Parkinson's" if correct_pred_probs[i] > threshold else "Healthy"
    confidence = correct_pred_probs[i]


    plt.title(f"{correct_img_names[i]}\nTrue: {true_label} | Pred: {pred_label}\nProb: {confidence:.2f}")

plt.tight_layout()
plt.show()

"""accuracy increased"""

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import ConvNeXtTiny
from tensorflow.keras.applications.convnext import preprocess_input
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

base_model = ConvNeXtTiny(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze base model layers

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu")(x)
x = Dense(1, activation="sigmoid")(x)  # Binary classification (Parkinson's vs Healthy)

model = Model(inputs=base_model.input, outputs=x)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

train_folder = "/content/drive/My Drive/training"
batch_size = 32

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    train_folder, target_size=(224, 224), batch_size=batch_size, class_mode="binary", subset="training"
)

val_generator = train_datagen.flow_from_directory(
    train_folder, target_size=(224, 224), batch_size=batch_size, class_mode="binary", subset="validation"
)

model.fit(train_generator, validation_data=val_generator, epochs=15)

test_folder = "/content/drive/My Drive/testing"

def load_and_preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = preprocess_input(img_array)
    return np.expand_dims(img_array, axis=0)  # Add batch dimension

print("Folders found in test dataset:", os.listdir(test_folder))

true_labels = []
pred_probs = []
img_paths = []
img_names = []

for class_name in os.listdir(test_folder):
    class_folder = os.path.join(test_folder, class_name)

    if not os.path.isdir(class_folder):
        continue

    if class_name.lower() == "parkinson":
        label = 1  # Parkinson's
    elif class_name.lower() == "healthy":
        label = 0  # Healthy
    else:
        print(f"Skipping unknown folder: {class_name}")
        continue  # Skip unknown folders

    for img_name in os.listdir(class_folder):
        img_path = os.path.join(class_folder, img_name)

        if img_path.lower().endswith((".jpg", ".jpeg", ".png")):
            img_array = load_and_preprocess_image(img_path)
            pred_prob = model.predict(img_array, verbose=0)[0][0]

            true_labels.append(label)
            pred_probs.append(pred_prob)
            img_paths.append(img_path)
            img_names.append(img_name)

true_labels = np.array(true_labels)
pred_probs = np.array(pred_probs)

# ✅ Step 6: Correctly Classified Images
threshold = 0.5  # Binary classification threshold
pred_labels = (pred_probs > threshold).astype(int)  # Convert probabilities to binary labels

correctly_classified_indices = np.where(true_labels == pred_labels)[0]  # Get indices of correct classifications

correct_img_paths = [img_paths[i] for i in correctly_classified_indices]
correct_img_names = [img_names[i] for i in correctly_classified_indices]
correct_true_labels = [true_labels[i] for i in correctly_classified_indices]
correct_pred_probs = [pred_probs[i] for i in correctly_classified_indices]

num_correct_images = len(correct_img_paths)
cols = 5  # Images per row
rows = (num_correct_images // cols) + (num_correct_images % cols > 0)

plt.figure(figsize=(20, rows * 4))

for i, img_path in enumerate(correct_img_paths):
    img = image.load_img(img_path, target_size=(224, 224))

    plt.subplot(rows, cols, i + 1)
    plt.imshow(img)
    plt.axis("off")

    true_label = "Parkinson's" if correct_true_labels[i] == 1 else "Healthy"
    pred_label = "Parkinson's" if correct_pred_probs[i] > threshold else "Healthy"
    confidence = correct_pred_probs[i]

    plt.title(f"{correct_img_names[i]}\nTrue: {true_label} | Pred: {pred_label}\nProb: {confidence:.2f}")

plt.tight_layout()
plt.show()

"""# ConvNeXt prediction"""

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import ConvNeXtTiny
from tensorflow.keras.applications.convnext import preprocess_input
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

base_model = ConvNeXtTiny(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu")(x)
x = Dense(1, activation="sigmoid")(x)

model = Model(inputs=base_model.input, outputs=x)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])


train_folder = "/content/drive/My Drive/training"
batch_size = 32

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    train_folder, target_size=(224, 224), batch_size=batch_size, class_mode="binary", subset="training"
)

val_generator = train_datagen.flow_from_directory(
    train_folder, target_size=(224, 224), batch_size=batch_size, class_mode="binary", subset="validation"
)


model.fit(train_generator, validation_data=val_generator, epochs=15)


test_folder = "/content/drive/My Drive/testing"

def load_and_preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = preprocess_input(img_array)
    return np.expand_dims(img_array, axis=0)  # Add batch dimension

print("Folders found in test dataset:", os.listdir(test_folder))

true_labels = []
pred_probs = []
img_paths = []
img_names = []

for class_name in os.listdir(test_folder):
    class_folder = os.path.join(test_folder, class_name)

    if not os.path.isdir(class_folder):
        continue

    if class_name.lower() == "parkinson":
        label = 1
    elif class_name.lower() == "healthy":
        label = 0
    else:
        print(f"Skipping unknown folder: {class_name}")
        continue

    for img_name in os.listdir(class_folder):
        img_path = os.path.join(class_folder, img_name)

        if img_path.lower().endswith((".jpg", ".jpeg", ".png")):
            img_array = load_and_preprocess_image(img_path)
            pred_prob = model.predict(img_array, verbose=0)[0][0]

            true_labels.append(label)
            pred_probs.append(pred_prob)
            img_paths.append(img_path)
            img_names.append(img_name)

true_labels = np.array(true_labels)
pred_probs = np.array(pred_probs)


threshold = 0.5
pred_labels = (pred_probs > threshold).astype(int)

correctly_classified_indices = np.where(true_labels == pred_labels)[0]

correct_img_paths = [img_paths[i] for i in correctly_classified_indices]
correct_img_names = [img_names[i] for i in correctly_classified_indices]
correct_true_labels = [true_labels[i] for i in correctly_classified_indices]
correct_pred_probs = [pred_probs[i] for i in correctly_classified_indices]

num_correct_images = len(correct_img_paths)
cols = 5
rows = (num_correct_images // cols) + (num_correct_images % cols > 0)

plt.figure(figsize=(20, rows * 4))

for i, img_path in enumerate(correct_img_paths):
    img = image.load_img(img_path, target_size=(224, 224))

    plt.subplot(rows, cols, i + 1)
    plt.imshow(img)
    plt.axis("off")

    true_label = "Parkinson's" if correct_true_labels[i] == 1 else "Healthy"
    pred_label = "Parkinson's" if correct_pred_probs[i] > threshold else "Healthy"
    confidence = correct_pred_probs[i]

    plt.title(f"{correct_img_names[i]}\nTrue: {true_label} | Pred: {pred_label}\nProb: {confidence:.2f}")

plt.tight_layout()
plt.show()


cm = confusion_matrix(true_labels, pred_labels)


plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Healthy", "Parkinson's"], yticklabels=["Healthy", "Parkinson's"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n", classification_report(true_labels, pred_labels, target_names=["Healthy", "Parkinson's"]))

